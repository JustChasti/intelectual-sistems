{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6DpaxaJITxc"
   },
   "source": [
    "**Цель:**\n",
    "\n",
    "Классификация последовательностей - это проблема прогнозирующего моделирования,\n",
    "когда у вас есть некоторая последовательность входных данных в пространстве или\n",
    "времени, и задача состоит в том, чтобы предсказать категорию для последовательности.\n",
    "Проблема усложняется тем, что последовательности могут различаться по длине,\n",
    "состоять из очень большого словарного запаса входных символов и могут потребовать от\n",
    "модели изучения долгосрочного контекста или зависимостей между символами во входной\n",
    "последовательности.\n",
    "В данной лабораторной работе также будет использоваться датасет IMDb, однако\n",
    "обучение будет проводиться с помощью рекуррентной нейронной сети.\n",
    "\n",
    "**Задачи:**\n",
    "\n",
    "Ознакомиться с рекуррентными нейронными сетями\n",
    "\n",
    "Изучить способы классификации текста\n",
    "\n",
    "Ознакомиться с ансамблированием сетей\n",
    "\n",
    "Построить ансамбль сетей, который позволит получать точность не менее 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2HDgC-_cFjtt"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "import tkinter as tk\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1013558,
     "status": "ok",
     "timestamp": 1649237833968,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "zSskb-xZF20T",
    "outputId": "77252312-f631-4b8c-bdbc-55149b6502ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 308s 775ms/step - loss: 0.4810 - accuracy: 0.7659 - val_loss: 0.3577 - val_accuracy: 0.8504\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 324s 830ms/step - loss: 0.3403 - accuracy: 0.8534 - val_loss: 0.3354 - val_accuracy: 0.8612\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 294s 751ms/step - loss: 0.2752 - accuracy: 0.8898 - val_loss: 0.3210 - val_accuracy: 0.8688\n",
      "Accuracy: 86.88%\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb  # Загружаем датесет IMDb,\n",
    "(training_data, training_targets), (testing_data,\n",
    "testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "\n",
    "targets = np.concatenate((training_targets, testing_targets),axis=0)  # объединяем, потом разделим по другому\n",
    "\n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "maxlen=max_review_length)\n",
    "\n",
    "\n",
    "embedding_vecor_length = 32  # На вход получает номера слов, а на выходе выдаёт их векторные представления\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length,\n",
    "input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "epochs=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 560697,
     "status": "ok",
     "timestamp": 1649238402158,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "-LcY_wO-Lvtr",
    "outputId": "c2dbeaa2-ca05-412e-b7f1-55ca6e52ce73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 250, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 143s 360ms/step - loss: 0.4297 - accuracy: 0.7910 - val_loss: 0.2891 - val_accuracy: 0.8803\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 139s 355ms/step - loss: 0.2560 - accuracy: 0.8984 - val_loss: 0.2800 - val_accuracy: 0.8860\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 139s 355ms/step - loss: 0.1990 - accuracy: 0.9258 - val_loss: 0.2956 - val_accuracy: 0.8772\n",
      "Accuracy: 87.72%\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data,\n",
    "testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "\n",
    "targets = np.concatenate((training_targets, testing_targets),\n",
    "axis=0)\n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "epochs=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1230992,
     "status": "ok",
     "timestamp": 1649675668508,
     "user": {
      "displayName": "Анна Киселева",
      "userId": "05197195060357673021"
     },
     "user_tz": -180
    },
    "id": "Nfb_24uFsah0",
    "outputId": "85b4d0b3-0aef-44e4-f536-2d787dec4c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 265s 336ms/step - loss: 0.3427 - accuracy: 0.8435 - val_loss: 0.1662 - val_accuracy: 0.9360\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 265s 339ms/step - loss: 0.2319 - accuracy: 0.9091 - val_loss: 0.1461 - val_accuracy: 0.9440\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 242s 310ms/step - loss: 0.2035 - accuracy: 0.9214 - val_loss: 0.1133 - val_accuracy: 0.9600\n",
      "Accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb # меняем отношение тренировочных и тестовых данных\n",
    "from keras.layers import Dropout\n",
    "top_words = 5000\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=top_words)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "X_test = data[:250]\n",
    "y_test = targets[:250]\n",
    "X_train = data[1:]\n",
    "y_train = targets[1:]\n",
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32 # длина вектора, в котором будут представляться слова\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length)) # плотный вектор\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(100)) # рекуррентный слой\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 12443,
     "status": "ok",
     "timestamp": 1650377682091,
     "user": {
      "displayName": "Анна Киселева",
      "userId": "05197195060357673021"
     },
     "user_tz": -180
    },
    "id": "D178KCsuEgpd",
    "outputId": "4d902ea6-b53a-442f-a9ff-418d6311b93f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEGATIVE 0.8860284686088562'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = imdb.get_word_index()\n",
    "def predict(txt:str):\n",
    "    txt = txt.lower()\n",
    "    txt1 = \"\"\n",
    "    for i in txt:\n",
    "        if('a'<=i<='z' or i==' '):\n",
    "            txt1+=i\n",
    "    txt1=txt1.split()\n",
    "    tokens=np.array([min(index.get(i, 5000),5000)+3 for i in txt1])\n",
    "    vector = sequence.pad_sequences([tokens], maxlen=max_review_length)\n",
    "    p=model.predict(vector)[0][0]\n",
    "    return \"POSITIVE \"+str(p) if p>0.5 else \"NEGATIVE \"+str(1-p)\n",
    "\n",
    "predict(\"very very bad film\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "лаба7",
   "provenance": [
    {
     "file_id": "/v2/external/notebooks/welcome.ipynb",
     "timestamp": 1649278923570
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
