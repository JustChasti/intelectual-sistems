{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Цель:**\n",
    "\n",
    "Рекуррентные нейронные сети также могут быть использованы в качестве генеративных \n",
    "моделей.\n",
    "Это означает, что в дополнение к тому, что они используются для прогнозных моделей \n",
    "(создания прогнозов), они могут изучать последовательности проблемы, а затем \n",
    "генерировать совершенно новые вероятные последовательности для проблемной \n",
    "области.\n",
    "Подобные генеративные модели полезны не только для изучения того, насколько хорошо \n",
    "модель выявила проблему, но и для того, чтобы узнать больше о самой проблемной \n",
    "области.\n",
    "\n",
    "**Задачи:**\n",
    "\n",
    "Ознакомиться с генерацией текста\n",
    "Ознакомиться с системой Callback в Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163783\n",
      "Total Vocab:  61\n",
      "Total Patterns:  163683\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.9797\n",
      "Epoch 1: loss improved from inf to 2.97969, saving model to weights-improvement-01-2.9797.hdf5\n",
      "1279/1279 [==============================] - 586s 456ms/step - loss: 2.9797\n",
      "Epoch 2/3\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.8079\n",
      "Epoch 2: loss improved from 2.97969 to 2.80788, saving model to weights-improvement-02-2.8079.hdf5\n",
      "1279/1279 [==============================] - 578s 452ms/step - loss: 2.8079\n",
      "Epoch 3/3\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.7289\n",
      "Epoch 3: loss improved from 2.80788 to 2.72892, saving model to weights-improvement-03-2.7289.hdf5\n",
      "1279/1279 [==============================] - 580s 453ms/step - loss: 2.7289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe7b9a3f40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', \n",
    "verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X, y, epochs=3, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163783\n",
      "Total Vocab:  61\n",
      "Total Patterns:  163683\n",
      "Seed:\n",
      "\" e.\n",
      "\n",
      "'she can't explain it,' said the gryphon hastily. 'go on with the next\n",
      "verse.'\n",
      "\n",
      "'but about his t \"\n",
      "oen ' said the morke  and the woite sart oo the woree sart oo the wooee \n",
      "and the woile sar toi wort oo the sar oo the tooee oa the sooee of the caree the was oo the tooee to the tooee th the tooee to the tooee th the tooee to the tooee th the tooee th the tooe      '                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "filename = \"weights8.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163783\n",
      "Total Vocab:  61\n",
      "Total Patterns:  163683\n",
      "640/640 [==============================] - ETA: 0s - loss: 4.2415"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 15:54:06.317 | DEBUG    | __main__:on_epoch_end:48 - Generated text:\n",
      "2022-05-03 15:54:06.318 | INFO     | __main__:on_epoch_end:49 -  ;;  ri f  ne  eid i    f;r r ;     nr                f ;      r ret   t r r; tr n  e d      ;         ;   ;;;r   tp  rr err ;    r  ; ;i rr;r  ; i r; ;;r r;r; r t e t  ;  ; ; r   r    ret  ; ;  ; i;   rtd  r  n i nt     r   n gt;     t n ;  ;; nnrt n td;r  d n   n e   ;   nr ;  g   e     r  ;rr  i            rn r      nn  ;er t ; ;  ;n ;  rn r; fr inr;i f r   n ;  r i r; ; tr  ;; r ;  ;n;    ;   h; i  ;       t;  ;             t     t       ;    r  r   i  r  tt;r t ; ;;f;t  r      ;ar  red; ;;;\n",
      "2022-05-03 15:54:06.319 | DEBUG    | __main__:on_epoch_end:50 - Info:\n",
      "2022-05-03 15:54:06.320 | INFO     | __main__:on_epoch_end:51 - saving model weights with loss: 4.241524696350098 and epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 679s 1s/step - loss: 4.2415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e313096700>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "class MyCallback(Callback):\n",
    "    def __init__(self, path=''):\n",
    "        super(MyCallback, self).__init__()\n",
    "        self.path = str(path)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        name = f'{self.path}/model_{epoch}.hdf5' \n",
    "        self.model.save_weights(name, overwrite=True)\n",
    "\n",
    "        filename = \"wonderland.txt\"\n",
    "        raw_text = open(filename).read()\n",
    "        raw_text = raw_text.lower()\n",
    "        chars = sorted(list(set(raw_text)))\n",
    "        char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "        n_chars = len(raw_text)\n",
    "        n_vocab = len(chars)\n",
    "        seq_length = 100\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        for i in range(0, n_chars - seq_length, 1):\n",
    "            seq_in = raw_text[i:i + seq_length]\n",
    "            seq_out = raw_text[i + seq_length]\n",
    "            dataX.append([char_to_int[char] for char in seq_in])\n",
    "            dataY.append(char_to_int[seq_out])\n",
    "        start = numpy.random.randint(0, len(dataX)-1)\n",
    "        pattern = dataX[start]\n",
    "        out_data = ''\n",
    "        for i in range(500):\n",
    "            x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "            x = x / float(n_vocab)\n",
    "            prediction = model.predict(x, verbose=0)\n",
    "            index = numpy.argmax(prediction)\n",
    "            result = int_to_char[index]\n",
    "            seq_in = [int_to_char[value] for value in pattern]\n",
    "            out_data += result\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:len(pattern)]\n",
    "        logger.debug(\"Generated text:\")\n",
    "        logger.info(out_data)\n",
    "        logger.debug(\"Info:\")\n",
    "        logger.info(f'saving model weights with loss: {logs[\"loss\"]} and epoch: {epoch}')\n",
    "\n",
    "\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "filepath=\"bw8\"\n",
    "checkpoint = MyCallback(filepath)\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X, y, epochs=1, batch_size=256, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163783\n",
      "Total Vocab:  61\n",
      "Total Patterns:  163683\n",
      "Epoch 1/4\n",
      "640/640 [==============================] - 734s 1s/step - loss: 3.0385\n",
      "Epoch 2/4\n",
      "640/640 [==============================] - 707s 1s/step - loss: 2.8784\n",
      "Epoch 3/4\n",
      "640/640 [==============================] - 679s 1s/step - loss: 2.7931\n",
      "Epoch 4/4\n",
      "640/640 [==============================] - 663s 1s/step - loss: 2.7299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe7b9398e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, Callback, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "class MyCallback(Callback):\n",
    "    def __init__(self, path=''):\n",
    "        super(MyCallback, self).__init__()\n",
    "        self.path = str(path)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        name = f'{self.path}/model_{epoch}.hdf5' \n",
    "        self.model.save_weights(name, overwrite=True)\n",
    "\n",
    "        filename = \"wonderland.txt\"\n",
    "        raw_text = open(filename).read()\n",
    "        raw_text = raw_text.lower()\n",
    "        chars = sorted(list(set(raw_text)))\n",
    "        char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "        n_chars = len(raw_text)\n",
    "        n_vocab = len(chars)\n",
    "        seq_length = 100\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        for i in range(0, n_chars - seq_length, 1):\n",
    "            seq_in = raw_text[i:i + seq_length]\n",
    "            seq_out = raw_text[i + seq_length]\n",
    "            dataX.append([char_to_int[char] for char in seq_in])\n",
    "            dataY.append(char_to_int[seq_out])\n",
    "        start = numpy.random.randint(0, len(dataX)-1)\n",
    "        pattern = dataX[start]\n",
    "        out_data = ''\n",
    "        for i in range(500):\n",
    "            x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "            x = x / float(n_vocab)\n",
    "            prediction = model.predict(x, verbose=0)\n",
    "            index = numpy.argmax(prediction)\n",
    "            result = int_to_char[index]\n",
    "            seq_in = [int_to_char[value] for value in pattern]\n",
    "            out_data += result\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:len(pattern)]\n",
    "        logger.debug(\"Generated text:\")\n",
    "        logger.info(out_data)\n",
    "        logger.debug(\"Info:\")\n",
    "        logger.info(f'saving model weights with loss: {logs[\"loss\"]} and epoch: {epoch}')\n",
    "\n",
    "\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# поменял код и изменил оптимизатор и батч сайз для теста MyCallback\n",
    "filepath=\"bw8\"\n",
    "tb_callback = TensorBoard('./logs')\n",
    "callbacks_list = [tb_callback]\n",
    "model.fit(X, y, epochs=4, batch_size=256, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустить в коммандной строке\n",
    "\n",
    "    tensorboard --logdir=D:\\study\\intelectual-sistems\\logs\\train\\"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "865d8b2eb28e274047ba64063dfb6a2aabf0dfec4905d304d7a76618dae6fdd4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
